the image i sent is the result of your code.
now i want you to make it so that it will detect face in an image like it. the image sent will be something like it. 
so first
in the BACKEND please make the image better. not that ugly. please find or use existing or new library to make the image better.
and when hitting the python service image. please make the image face recognition in python to also handle that image like that.
remember that the arduino c++ code is not wrong. the problem is backend. please improve it far far further.
or when the arduino send data. it can also send some data that will help the backend make the image far far better.
basically the arduino right now is fine, but if there's a way to upgrade the image in the BACKEND, but using data from arduino, you can send it. but mostly modify the backend and python to handle it.

this is my `` : 
```
const express = require('express');
const cors = require('cors');
const helmet = require('helmet');
const rateLimit = require('express-rate-limit');
const morgan = require('morgan');
const WebSocket = require('ws');
const dotenv = require('dotenv');
const { createSshTunnel } = require('../job/tunnel');

// Import our separated modules
const { DataStore } = require('./dataStore');
const setupRoutes = require('./routes');

// Load environment variables
dotenv.config();

// Create Express app
const app = express();
const port = process.env.PORT || 3000;

app.use(cors('*')); // Allow all origins for simplicity, adjust as needed
app.use(helmet());
app.use(express.json());
app.use(morgan('dev'));

// Rate limiting
const apiLimiter = rateLimit({
  windowMs: process.env.RATE_LIMIT_WINDOW_MS || 60000,
  max: process.env.RATE_LIMIT_MAX || 100,
  message: 'Too many requests, please try again later'
});
app.use('/api/', apiLimiter);

// Initialize data store
const dataStore = new DataStore();

// WebSocket server
const wss = new WebSocket.Server({ noServer: true });

wss.on('connection', (ws) => {
  ws.on('message', (message) => {
    console.log('Received WebSocket message:', message.toString());
  });

  ws.send(JSON.stringify({
    type: 'connection',
    status: 'connected',
    timestamp: Date.now()
  }));
});

// Setup routes with dependencies
setupRoutes(app, dataStore, wss);

// Start server
const server = app.listen(port, () => {
  console.log(`IoT Backend running on port ${port}`);
});

// Handle WebSocket upgrades
server.on('upgrade', (request, socket, head) => {
  wss.handleUpgrade(request, socket, head, (ws) => {
    wss.emit('connection', ws, request);
  });
});

// SSH tunnel setup if configured
if (process.env.PUBLIC_VPS_IP) {
  const sshClient = createSshTunnel();
  if (sshClient) {
    console.log('SSH tunnel established');
  }
}

module.exports = server;
```

this is my `iot-backend-express\src\routes.js` : 
```
const express = require('express');
const multer = require('multer');
const WebSocket = require('ws');
const fs = require('fs');
const fsp = require('fs').promises;
const path = require('path');
const os = require('os');
const ffmpeg = require('fluent-ffmpeg');
const sharp = require('sharp');
const Jimp = require('jimp'); // Add Jimp for better RGB565 handling

const { dataDir, recordingsDir } = require('./dataStore');

// Placeholder data for demo purposes
let devices = [
  { id: "device1", name: "Living Room Cam", ipAddress: "192.168.1.101", uptime: "12h 30m", freeHeap: "512KB", lastSeen: new Date().toISOString(), errors: 0, capabilities: ["stream", "record", "ptz"], status: "online" },
  { id: "device2", name: "Kitchen Sensor", ipAddress: "192.168.1.102", uptime: "2d 5h", freeHeap: "1024KB", lastSeen: new Date().toISOString(), errors: 1, capabilities: ["temperature", "humidity"], status: "warning" },
  { id: "device3", name: "Garage Door", ipAddress: "192.168.1.103", uptime: "5h 15m", freeHeap: "256KB", lastSeen: new Date(Date.now() - 3600000 * 3).toISOString(), errors: 0, capabilities: ["open", "close", "status"], status: "offline" },
];

let systemStatus = {
  totalDevices: devices.length,
  onlineDevices: devices.filter(d => d.status === "online").length,
  systemLoad: "Low",
  storageUsage: "45%",
  lastBackup: new Date(Date.now() - 86400000).toISOString(),
};

// Multer configurations
const streamUpload = multer({ storage: multer.memoryStorage() });
const permittedFaceUpload = multer({ storage: multer.memoryStorage() });

function setupRoutes(app, dataStore, wss) {
  // Health check
  app.get('/health', (req, res) => {
    res.json({ 
      status: 'healthy', 
      uptime: process.uptime(), 
      timestamp: Date.now() 
    });
  });

  // Device registration
  app.post('/api/v1/devices/register', (req, res) => {
    const device = req.body;
    if (!device.deviceId || !device.deviceName || !device.deviceType) {
      return res.status(400).json({ error: 'Missing required fields' });
    }
    const registeredDevice = dataStore.registerDevice({
      id: device.deviceId,
      name: device.deviceName,
      type: device.deviceType,
      ipAddress: device.ipAddress,
      status: 'online',
      lastSeen: Date.now(),
      uptime: 0,
      freeHeap: 0,
      capabilities: device.capabilities || []
    });
    res.json(registeredDevice);
  });

  // Heartbeat endpoint
  app.post('/api/v1/devices/heartbeat', (req, res) => {
    const { deviceId, uptime, freeHeap, wifiRssi, status } = req.body;
    if (!deviceId) {
      return res.status(400).json({ error: 'Device ID required' });
    }
    const device = dataStore.getDevice(deviceId);
    if (device) {
      device.lastSeen = Date.now();
      device.uptime = uptime || device.uptime;
      device.freeHeap = freeHeap || device.freeHeap;
      device.wifiRssi = wifiRssi;
      device.status = status || 'online';
      dataStore.registerDevice(device);
    }
    res.json({ message: 'Heartbeat received', status: 'success' });
  });

  // Get all devices
  app.get('/api/v1/devices', (req, res) => {
    const allDevices = dataStore.getAllDevices();
    const formattedDevices = allDevices.map(device => ({
      deviceId: device.id,
      deviceName: device.name,
      deviceType: device.type,
      status: device.status,
      ipAddress: device.ipAddress,
      lastHeartbeat: device.lastSeen,
      uptime: device.uptime,
      freeHeap: device.freeHeap,
      wifiRssi: device.wifiRssi,
      errorCount: device.errors || 0,
      capabilities: device.capabilities || []
    }));
    res.json({
      success: true,
      devices: formattedDevices
    });
  });

  // Get system status
  app.get('/api/v1/system/status', (req, res) => {
    const allDevices = dataStore.getAllDevices();
    const onlineDevices = allDevices.filter(d => d.status === 'online' || d.status === 'warning').length;
    const systemStatus = {
      devicesOnline: onlineDevices,
      devicesTotal: allDevices.length,
      systemUptime: Math.floor(process.uptime()),
      totalCommandsSent: 0,
      totalCommandsFailed: 0,
      backendConnected: true,
      lastBackendSync: Date.now(),
      systemLoad: 0.1
    };
    res.json({
      success: true,
      status: systemStatus
    });
  });

  // Sensor data ingestion
  app.post('/api/v1/ingest/sensor-data', (req, res) => {
    const sensorData = req.body;
    if (!sensorData.deviceId || !sensorData.timestamp) {
      return res.status(400).json({ error: 'Missing required fields' });
    }
    
    const savedData = dataStore.saveSensorData(sensorData);
    
    // Broadcast to WebSocket clients
    wss.clients.forEach(client => {
      if (client.readyState === WebSocket.OPEN) {
        client.send(JSON.stringify({
          type: 'sensor-data',
          data: savedData,
          timestamp: Date.now()
        }));
      }
    });
    
    res.json({ message: 'Data received', data: savedData });
  });

  // Stream endpoint with HYBRID processing (both JPEG and RAW support)
  app.post('/api/v1/stream/stream', async (req, res) => {
    const contentType = req.headers['content-type'] || '';
    const timestamp = Date.now();
    const deviceId = req.headers['x-device-id'] || 'unknown_device';
    const filename = `${deviceId}_${timestamp}.jpg`;
    const filePath = path.join(dataDir, filename);

    console.log(`[Stream API] Received request from ${deviceId}, Content-Type: ${contentType}`);

    try {
      let jpegBuffer;

      if (contentType === 'application/octet-stream') {
        // --- RAW RGB565 PROCESSING ---
        const width = parseInt(req.headers['x-frame-width'], 10);
        const height = parseInt(req.headers['x-frame-height'], 10);
        const format = req.headers['x-frame-format'];

        if (!width || !height || !format) {
          return res.status(400).json({ error: 'Missing frame metadata headers for raw data' });
        }

        console.log(`[Stream API] Processing RAW ${format} frame: ${width}x${height}`);

        // Parse raw body manually
        const rawBodyChunks = [];
        req.on('data', chunk => rawBodyChunks.push(chunk));
        req.on('end', async () => {
          try {
            const rawBuffer = Buffer.concat(rawBodyChunks);
            console.log(`[Stream API] Raw buffer size: ${rawBuffer.length} bytes`);

            if (format === 'RGB565') {
              const rgbBuffer = convertRGB565ToRGB(rawBuffer, width, height);
              jpegBuffer = await sharp(rgbBuffer, {
                raw: { width, height, channels: 3 }
              }).jpeg({ quality: 85 }).toBuffer();
            }

            await fsp.writeFile(filePath, jpegBuffer);
            console.log('[Stream API] Raw frame converted and saved:', filePath);

            const recognition = await dataStore.performFaceRecognition(jpegBuffer);
            
            // Broadcast and respond
            const wsMessage = {
              type: 'new_frame',
              deviceId, timestamp, filename,
              url: `/data/${filename}`,
              recognition: {
                status: recognition.status,
                recognizedAs: recognition.recognizedAs,
                confidence: recognition.confidence,
                error: recognition.error
              }
            };

            wss.clients.forEach(client => {
              if (client.readyState === WebSocket.OPEN) {
                client.send(JSON.stringify(wsMessage));
              }
            });

            res.json({ 
              message: 'Raw frame processed successfully', 
              filename, 
              recognitionStatus: recognition.status,
              recognizedAs: recognition.recognizedAs,
              confidence: recognition.confidence 
            });

          } catch (error) {
            console.error('[Stream API] Error processing raw frame:', error);
            res.status(500).json({ error: 'Failed to process raw frame', details: error.message });
          }
        });

      } else if (contentType.includes('multipart/form-data')) {
        // --- JPEG MULTIPART PROCESSING ---
        const upload = multer({ storage: multer.memoryStorage() }).single('image');
        
        upload(req, res, async (err) => {
          if (err || !req.file) {
            console.log('[Stream API] No image in multipart request');
            return res.status(400).json({ error: 'No image provided' });
          }

          console.log(`[Stream API] Processing JPEG frame: ${req.file.size} bytes`);

          try {
            // Save the JPEG directly (it's already compressed)
            await fsp.writeFile(filePath, req.file.buffer);
            console.log('[Stream API] JPEG frame saved:', filePath);

            // Perform face recognition
            const recognition = await dataStore.performFaceRecognition(req.file.buffer);
            console.log('[Stream API] Face recognition result:', JSON.stringify(recognition));
            
            // Broadcast to WebSocket clients
            const wsMessage = {
              type: 'new_frame',
              deviceId, timestamp, filename,
              url: `/data/${filename}`,
              recognition: {
                status: recognition.status,
                recognizedAs: recognition.recognizedAs,
                confidence: recognition.confidence,
                error: recognition.error
              }
            };
            
            wss.clients.forEach(client => {
              if (client.readyState === WebSocket.OPEN) {
                client.send(JSON.stringify(wsMessage));
              }
            });

            res.json({ 
              message: 'JPEG frame received and processed', 
              filename,
              recognitionStatus: recognition.status,
              recognizedAs: recognition.recognizedAs,
              confidence: recognition.confidence,
              error: recognition.error
            });

          } catch (error) {
            console.error('[Stream API] Error processing JPEG frame:', error);
            res.status(500).json({ error: 'Failed to process JPEG frame', details: error.message });
          }
        });

      } else {
        return res.status(400).json({ error: 'Unsupported content type. Expected multipart/form-data or application/octet-stream.' });
      }

    } catch (err) {
      console.error('[Stream API] General error:', err);
      res.status(500).json({ error: 'Internal server error', details: err.message });
    }
  });

  // Record video from last 30 seconds of frames
  app.post('/api/v1/stream/record', async (req, res) => {
    let tempDir = null;
    try {
      const filesInDataDir = await fsp.readdir(dataDir);
      const now = Date.now();
      const maxAgeMs = 30 * 1000; // 30 seconds
      let frameCount = 0;

      const imageFilesToProcess = [];

      for (const file of filesInDataDir) {
        const filePath = path.join(dataDir, file);
        try {
          const stat = await fsp.stat(filePath);
          if (!stat.isFile() || !file.toLowerCase().endsWith('.jpg')) continue;

          const parts = file.split('_');
          if (parts.length < 2) continue;

          const timestampStrWithExt = parts[parts.length - 1];
          const timestampStr = timestampStrWithExt.split('.')[0];
          const fileTimestamp = parseInt(timestampStr, 10);

          if (!isNaN(fileTimestamp) && (now - fileTimestamp <= maxAgeMs) && (now - fileTimestamp >= 0)) {
            imageFilesToProcess.push({ filePath, timestamp: fileTimestamp, originalName: file });
          }
        } catch (statError) {
          console.error(`Error stating file ${file}:`, statError);
        }
      }

      if (imageFilesToProcess.length === 0) {
        return res.json({ success: false, message: 'No frames found in the last 30 seconds to record.' });
      }

      // Sort files by timestamp
      imageFilesToProcess.sort((a, b) => a.timestamp - b.timestamp);

      // Create temporary directory for ffmpeg
      tempDir = await fsp.mkdtemp(path.join(os.tmpdir(), 'recording-frames-'));

      // Copy sorted files to tempDir with sequential names
      for (let i = 0; i < imageFilesToProcess.length; i++) {
        const item = imageFilesToProcess[i];
        const tempFileName = `img-${String(i).padStart(5, '0')}.jpg`;
        await fsp.copyFile(item.filePath, path.join(tempDir, tempFileName));
      }
      frameCount = imageFilesToProcess.length;

      const recordingId = `rec_${Date.now()}.mp4`;
      const outputVideoPath = path.join(recordingsDir, recordingId);
      const inputPattern = path.join(tempDir, 'img-%05d.jpg');

      const fps = Math.max(1, Math.min(30, Math.round(frameCount / 30))) || 10;

      await new Promise((resolve, reject) => {
        ffmpeg(inputPattern)
          .inputFPS(fps)
          .outputOptions([
            '-c:v libx264',
            '-pix_fmt yuv420p',
            '-movflags +faststart'
          ])
          .output(outputVideoPath)
          .on('end', () => {
            console.log(`Recording ${recordingId} saved with ${frameCount} frames at ${fps} FPS.`);
            resolve();
          })
          .on('error', (err) => {
            console.error('Error during ffmpeg processing:', err.message);
            reject(new Error(`FFmpeg error: ${err.message}`));
          })
          .run();
      });

      res.json({ success: true, data: { recordingId, frameCount, videoUrl: `/recordings/${recordingId}` } });

    } catch (error) {
      console.error('Error saving video recording:', error);
      res.status(500).json({ success: false, error: `Failed to save video recording: ${error.message}` });
    } finally {
      if (tempDir) {
        try {
          await fsp.rm(tempDir, { recursive: true, force: true });
          console.log('Temporary directory cleaned up:', tempDir);
        } catch (cleanupError) {
          console.error('Error cleaning up temporary directory:', cleanupError);
        }
      }
    }
  });

  // Get all recordings with consistent response format
  app.get('/api/v1/stream/recordings', async (req, res) => {
    try {
      // Set cache headers to ensure fresh data
      res.set({
        'Cache-Control': 'no-cache, no-store, must-revalidate',
        'Pragma': 'no-cache',
        'Expires': '0'
      });

      const files = await fsp.readdir(recordingsDir);
      const videoRecordingsPromises = files
        .filter(file => file.toLowerCase().endsWith('.mp4'))
        .map(async (file) => {
          const filePath = path.join(recordingsDir, file);
          try {
            const stats = await fsp.stat(filePath);
            let createdAt = stats.birthtime || stats.mtime;
            const match = file.match(/rec_(\d+)\.mp4/);
            if (match && match[1]) {
              createdAt = new Date(parseInt(match[1], 10));
            }
            return {
              id: file,
              name: file,
              url: `/recordings/${file}`,
              createdAt: createdAt.toISOString(),
              size: stats.size,
              type: 'video',
            };
          } catch (statError) {
            console.error(`Failed to get stats for ${filePath}:`, statError);
            return null;
          }
        });

      const videoRecordings = (await Promise.all(videoRecordingsPromises))
        .filter(Boolean)
        .sort((a, b) => new Date(b.createdAt).getTime() - new Date(a.createdAt).getTime());
      // Respond with the standardized object
      res.json({ success: true, data: videoRecordings });
    } catch (err) {
      console.error("Error reading recordings directory:", err);
      res.status(500).json({ success: false, error: 'Failed to retrieve recordings' });
    }
  });

  // Delete a recording
  app.delete('/api/v1/stream/recordings/:filename', async (req, res) => {
    const { filename } = req.params;
    
    if (!filename || !filename.endsWith('.mp4')) {
      return res.status(400).json({ success: false, error: 'Invalid filename provided' });
    }

    const filePath = path.join(recordingsDir, filename);

    try {
      // Check if file exists first
      await fsp.access(filePath);
      
      // Delete the file
      await fsp.unlink(filePath);
      console.log(`Recording ${filename} deleted successfully`);
      
      res.json({ success: true, message: `Recording ${filename} deleted successfully` });
    } catch (err) {
      if (err.code === 'ENOENT') {
        return res.status(404).json({ success: false, error: 'Recording not found' });
      }
      console.error(`Error deleting recording ${filename}:`, err);
      res.status(500).json({ success: false, error: 'Failed to delete recording' });
    }
  });

  // Get all image frames with consistent response format
  app.get('/api/v1/stream/frames', async (req, res) => {
    try {
      // Set cache headers to ensure fresh data
      res.set({
        'Cache-Control': 'no-cache, no-store, must-revalidate',
        'Pragma': 'no-cache',
        'Expires': '0'
      });

      const files = await fsp.readdir(dataDir);
      const imageFramesPromises = files
        .filter(file => file.toLowerCase().endsWith('.jpg') || file.toLowerCase().endsWith('.jpeg'))
        .map(async file => {
          const filePath = path.join(dataDir, file);
          try {
            const stats = await fsp.stat(filePath);
            return {
              id: file,
              name: file,
              url: `/data/${file}`,
              createdAt: stats.birthtime.toISOString(),
              size: stats.size,
              type: 'image',
            };
          } catch (statError) {
            console.error(`Error getting stats for image file ${file}:`, statError);
            return null;
          }
        });

        const imageFrames = (await Promise.all(imageFramesPromises))
            .filter(Boolean)
            .sort((a, b) => new Date(b.createdAt).getTime() - new Date(a.createdAt).getTime());

      // Respond with the standardized object
      res.json({ success: true, data: imageFrames });
    } catch (err) {
      console.error("Error reading data directory for frames:", err);
      res.status(500).json({ success: false, error: 'Failed to retrieve image frames' });
    }
  });

  // Add permitted face
  app.post('/api/v1/recognition/add-permitted-face', permittedFaceUpload.single('image'), async (req, res) => {
    if (!req.file) {
      return res.status(400).json({ error: 'No image provided for permitted face.' });
    }

    const subjectName = req.body.name || `subject_${Date.now()}`;

    try {
      const result = await dataStore.addPermittedFace(req.file.buffer, subjectName);
      res.json({ 
        success: true, 
        message: `Permitted face '${result.subjectName}' added successfully.`, 
        filename: result.filename 
      });
    } catch (error) {
      console.error("Error adding permitted face:", error);
      res.status(500).json({ 
        error: 'Failed to add permitted face.', 
        details: error.message 
      });
    }
  });

  // Legacy device routes (placeholder data)
  app.get('/api/v1/devices', (req, res) => {
    res.json(devices);
  });

  app.get('/api/v1/system/status', (req, res) => {
    systemStatus.totalDevices = devices.length;
    systemStatus.onlineDevices = devices.filter(d => d.status === "online" || d.status === "warning").length;
    res.json(systemStatus);
  });

  app.post('/api/v1/devices/:deviceId/command', (req, res) => {
    const { deviceId } = req.params;
    const { command, params } = req.body;

    const device = dataStore.getDevice(deviceId);
    if (!device) {
      return res.status(404).json({ success: false, message: "Device not found" });
    }

    console.log(`Received command '${command}' for device ${deviceId} with params:`, params);

    switch (command) {
      case 'restart':
        device.status = 'maintenance';
        dataStore.registerDevice(device);
        setTimeout(() => {
          const d = dataStore.getDevice(deviceId);
          if (d) {
            d.status = 'online';
            dataStore.registerDevice(d);
          }
        }, 5000);
        break;
      case 'ping':
        break;
      case 'snapshot':
        if (device.type === 'camera') {
          console.log(`Taking snapshot for camera ${deviceId}`);
        }
        break;
      default:
        console.log(`Unknown command: ${command}`);
    }

    res.json({
      success: true,
      message: `Command '${command}' sent to device ${deviceId} successfully.`,
      deviceStatus: device.status
    });
  });

  // Serve static files
  app.use('/data', express.static(dataDir));
  app.use('/recordings', express.static(recordingsDir));

  // Debug utility for RGB565 conversion testing
  function debugRGB565Conversion(rgb565Buffer, width, height, samplePixels = 5) {
    console.log(`[RGB565 Debug] Buffer size: ${rgb565Buffer.length} bytes`);
    console.log(`[RGB565 Debug] Expected size: ${width * height * 2} bytes`);
    console.log(`[RGB565 Debug] Dimensions: ${width}x${height}`);
    
    // Sample a few pixels for debugging
    for (let i = 0; i < Math.min(samplePixels * 2, rgb565Buffer.length); i += 2) {
      const rgb565 = rgb565Buffer.readUInt16LE(i);
      
      const r5 = (rgb565 >> 11) & 0x1F;
      const g6 = (rgb565 >> 5) & 0x3F;
      const b5 = rgb565 & 0x1F;
      
      const r8 = Math.round((r5 * 255) / 31);
      const g8 = Math.round((g6 * 255) / 63);
      const b8 = Math.round((b5 * 255) / 31);
      
      console.log(`[RGB565 Debug] Pixel ${i/2}: RGB565=0x${rgb565.toString(16).padStart(4, '0')} â†’ RGB(${r8},${g8},${b8})`);
    }
  }

  // Improved RGB565 to RGB conversion function
  function convertRGB565ToRGB(rgb565Buffer, width, height) {
    console.log(`[RGB565] Converting ${width}x${height} RGB565 buffer (${rgb565Buffer.length} bytes)`);
    
    const rgbBuffer = Buffer.alloc(width * height * 3);
    const expectedSize = width * height * 2; // RGB565 is 2 bytes per pixel
    
    if (rgb565Buffer.length !== expectedSize) {
      console.warn(`[RGB565] Warning: Buffer size mismatch. Expected ${expectedSize}, got ${rgb565Buffer.length}`);
    }
    
    let rgbIndex = 0;
    
    for (let i = 0; i < Math.min(rgb565Buffer.length, expectedSize); i += 2) {
      // Read RGB565 as big-endian (ESP32 might send big-endian)
      const rgb565 = rgb565Buffer.readUInt16BE(i);
      
      // Extract 5-6-5 bits correctly
      const r5 = (rgb565 & 0xF800) >> 11;  // Bits 15-11 (5 bits red)
      const g6 = (rgb565 & 0x07E0) >> 5;   // Bits 10-5  (6 bits green)  
      const b5 = (rgb565 & 0x001F);        // Bits 4-0   (5 bits blue)
      
      // Convert to 8-bit values with proper scaling
      const r8 = Math.round((r5 * 255) / 31);
      const g8 = Math.round((g6 * 255) / 63);
      const b8 = Math.round((b5 * 255) / 31);
      
      // Store in RGB buffer
      rgbBuffer[rgbIndex++] = r8;
      rgbBuffer[rgbIndex++] = g8;
      rgbBuffer[rgbIndex++] = b8;
    }
    
    return rgbBuffer;
  }

  // Alternative RGB565 conversion using little-endian
  function convertRGB565ToRGBLE(rgb565Buffer, width, height) {
    console.log(`[RGB565-LE] Converting ${width}x${height} RGB565 buffer (${rgb565Buffer.length} bytes)`);
    
    const rgbBuffer = Buffer.alloc(width * height * 3);
    const expectedSize = width * height * 2;
    
    let rgbIndex = 0;
    
    for (let i = 0; i < Math.min(rgb565Buffer.length, expectedSize); i += 2) {
      // Read RGB565 as little-endian
      const rgb565 = rgb565Buffer.readUInt16LE(i);
      
      // Extract 5-6-5 bits correctly
      const r5 = (rgb565 & 0xF800) >> 11;
      const g6 = (rgb565 & 0x07E0) >> 5;
      const b5 = (rgb565 & 0x001F);
      
      // Convert to 8-bit values
      const r8 = Math.round((r5 * 255) / 31);
      const g8 = Math.round((g6 * 255) / 63);
      const b8 = Math.round((b5 * 255) / 31);
      
      rgbBuffer[rgbIndex++] = r8;
      rgbBuffer[rgbIndex++] = g8;
      rgbBuffer[rgbIndex++] = b8;
    }
    
    return rgbBuffer;
  }
}

module.exports = setupRoutes;
```

this is my `iot-backend-express\src\dataStore.js` : 
```
const fs = require('fs');
const fsp = require('fs').promises;
const path = require('path');

// Ensure directories exist
const dataDir = path.join(__dirname, '../data');
const recordingsDir = path.join(__dirname, '../recordings');
const permittedFacesDir = path.join(__dirname, '../permitted_faces');

[dataDir, recordingsDir, permittedFacesDir].forEach(dir => {
  if (!fs.existsSync(dir)) {
    fs.mkdirSync(dir, { recursive: true });
  }
});

// Simplified face recognition - delegated to Python service
let faceRecognitionEnabled = false;

// Function to clean up old recordings
function cleanupOldRecordings(directory, maxAgeMs) {
  fs.readdir(directory, (err, files) => {
    if (err) {
      console.error("Error reading data directory for cleanup:", err);
      return;
    }

    const now = Date.now();
    files.forEach(file => {
      const parts = file.split('_');
      if (parts.length >= 2) {
        const timestampStr = parts[parts.length - 1].split('.')[0];
        const timestamp = parseInt(timestampStr, 10);
        if (!isNaN(timestamp) && (now - timestamp > maxAgeMs)) {
          const filePath = path.join(directory, file);
          fs.unlink(filePath, unlinkErr => {
            if (unlinkErr) {
              console.error(`Error deleting old file ${filePath}:`, unlinkErr);
            } else {
              console.log(`Deleted old file: ${filePath}`);
            }
          });
        }
      }
    });
  });
}

// Data store implementation
class DataStore {
  constructor() {
    this.devices = new Map();
    this.sensorData = new Map();
    this.commands = new Map();
    this.notes = new Map();
    this.nextNoteId = 1;
    
    console.log('DataStore initialized - Face recognition delegated to Python service');
    
    // Setup automatic cleanup
    const CLEANUP_INTERVAL_MS = 5 * 60 * 1000; // 5 minutes
    const FRAME_MAX_AGE_MS = 10 * 60 * 1000;   // 10 minutes
    
    setInterval(() => {
      console.log(`Running scheduled cleanup of old frames (older than ${FRAME_MAX_AGE_MS / 60000} mins)...`);
      cleanupOldRecordings(dataDir, FRAME_MAX_AGE_MS);
    }, CLEANUP_INTERVAL_MS);
  }

  // Device operations
  updateDevice(deviceId, updates) {
    const device = this.devices.get(deviceId);
    if (device) {
      Object.assign(device, updates, { lastSeen: Date.now() });
      this.devices.set(deviceId, device);
      return device;
    }
    return null;
  }

  registerDevice(device) {
    const existingDevice = this.devices.get(device.id);
    if (existingDevice) {
      const updatedDevice = {
        ...existingDevice,
        ...device,
        lastSeen: Date.now()
      };
      this.devices.set(device.id, updatedDevice);
      return updatedDevice;
    } else {
      const newDevice = {
        ...device,
        status: device.status || 'online',
        lastSeen: Date.now(),
        errors: 0
      };
      this.devices.set(device.id, newDevice);
      return newDevice;
    }
  }

  getDevice(deviceId) {
    return this.devices.get(deviceId);
  }

  getAllDevices() {
    return Array.from(this.devices.values());
  }

  getSystemStatus() {
    return {
      devicesOnline: this.getAllDevices().filter(d => d.status === 'online').length,
      devicesTotal: this.devices.size,
      uptime: process.uptime(),
      timestamp: Date.now()
    };
  }

  getDeviceStatusSummary() {
    const devices = this.getAllDevices();
    return {
      total: devices.length,
      online: devices.filter(d => d.status === 'online').length,
      offline: devices.filter(d => d.status === 'offline').length,
      warning: devices.filter(d => d.status === 'warning').length,
      error: devices.filter(d => d.status === 'error').length
    };
  }

  // Sensor data operations
  saveSensorData(data) {
    if (!this.sensorData.has(data.deviceId)) {
      this.sensorData.set(data.deviceId, []);
    }
    this.sensorData.get(data.deviceId).push(data);
    return data;
  }

  // GPU-accelerated face recognition via Python service
  async performFaceRecognition(imageBuffer) {
    const pythonServiceUrl = process.env.PYTHON_GPU_SERVICE_URL || 'http://localhost:9001';
    const serviceEnabled = process.env.PYTHON_GPU_SERVICE_ENABLED !== 'false';

    console.log(`[Face Recognition] Service URL: ${pythonServiceUrl}/recognize, Enabled: ${serviceEnabled}`);

    if (!serviceEnabled) {
      console.log('[Face Recognition] Service is disabled in configuration.');
      return {
        status: 'service_disabled',
        recognizedAs: null,
        error: 'Face recognition service is disabled.'
      };
    }

    try {
      const FormData = require('form-data');
      // Ensure you have node-fetch installed: npm install node-fetch
      const { default: fetch } = await import('node-fetch');
      
      const form = new FormData();
      form.append('image', imageBuffer, { // This field name must match the FastAPI endpoint
        filename: 'frame.jpg',
        contentType: 'image/jpeg'
      });
      
      console.log(`[Face Recognition] Attempting to call Python GPU service at: ${pythonServiceUrl}/api/v1/recognize`);

      const response = await fetch(`${pythonServiceUrl}/api/v1/recognize`, {
        method: 'POST',
        body: form,
        headers: form.getHeaders(),
        timeout: 15000 // Increased timeout to 15 seconds
      });
      
      console.log(`[Face Recognition] Response status from Python service: ${response.status}`);

      if (response.ok) {
        const result = await response.json();
        console.log('[Face Recognition] Raw result from Python service:', result);
        
        // Adapt based on the actual structure of a successful response from your Python service
        // This structure assumes the Python service might return 'status', 'recognizedAs', 'confidence', 'faces_detected', 'processing_time'
        // Or for older/different versions: 'status', 'recognized_faces' (array), 'faces_detected', 'processing_time'
        if (result.status === 'permitted_face' || (result.status === 'success' && result.recognized_faces && result.recognized_faces.length > 0)) {
          const faceName = result.recognizedAs || (result.recognized_faces && result.recognized_faces[0] ? result.recognized_faces[0].name : 'Unknown');
          const confidence = result.confidence || (result.recognized_faces && result.recognized_faces[0] ? result.recognized_faces[0].confidence : null);
          return {
            status: 'recognized',
            recognizedAs: faceName,
            confidence: confidence,
            faces_detected: result.faces_detected,
            processing_time: result.processing_time
          };
        } else if (result.status === 'unknown_face' || (result.status === 'success' && result.faces_detected > 0 && (!result.recognized_faces || result.recognized_faces.length === 0))) {
          return {
            status: 'unknown_face',
            recognizedAs: null,
            faces_detected: result.faces_detected,
            processing_time: result.processing_time
          };
        } else if (result.status === 'no_face_detected' || result.status === 'no_faces' || (result.status === 'success' && result.faces_detected === 0)) {
           return {
            status: 'no_faces',
            recognizedAs: null,
            faces_detected: 0,
            processing_time: result.processing_time
          };
        } else {
            console.warn('[Face Recognition] Unexpected success response structure or status from Python service:', result);
            return {
                status: result.status || 'unexpected_response',
                recognizedAs: null,
                error: 'Unexpected response structure from recognition service.',
                details: result
            };
        }
      } else {
        const errorBody = await response.text();
        console.error(`[Face Recognition] Python service at ${pythonServiceUrl}/recognize returned error ${response.status}: ${errorBody}`);
        return {
          status: 'service_error',
          recognizedAs: null,
          error: `Recognition service returned ${response.status}`,
          details: errorBody
        };
      }
      
    } catch (err) {
      console.error(`[Face Recognition] Error calling Python GPU service at ${pythonServiceUrl}/recognize: ${err.message}`, err);
      if (err.name === 'AbortError' || err.message.toLowerCase().includes('timeout')) {
        return {
          status: 'service_timeout',
          recognizedAs: null,
          error: `Request to recognition service timed out: ${err.message}`
        };
      }
      return {
        status: 'service_error',
        recognizedAs: null,
        error: `Failed to connect to recognition service: ${err.message}`
      };
    }
  }

  async addPermittedFace(imageBuffer, subjectName) {
    try {
      // Call Python service to add permitted face
      const FormData = require('form-data');
      const { default: fetch } = await import('node-fetch');
      
      // Get Python service URL from environment
      const pythonServiceUrl = process.env.PYTHON_GPU_SERVICE_URL || 'http://localhost:9001';
      
      const form = new FormData();
      form.append('image', imageBuffer, {
        filename: 'permitted_face.jpg',
        contentType: 'image/jpeg'
      });
      form.append('name', subjectName);
      
      const response = await fetch(`${pythonServiceUrl}/api/v1/recognition/add-permitted-face`, {
        method: 'POST',
        body: form,
        headers: form.getHeaders()
      });
      
      if (response.ok) {
        const result = await response.json();
        return result;
      } else {
        throw new Error('Failed to add permitted face via Python service');
      }
    } catch (error) {
      console.error("Error adding permitted face:", error);
      throw error;
    }
  }
}

module.exports = {
  DataStore,
  dataDir,
  recordingsDir,
  permittedFacesDir,
  cleanupOldRecordings
};
```

this is my `iot-backend-express\.env.example` : 
```
# Server Configuration
PORT=3001
HOST=0.0.0.0
NODE_ENV=development

# Security
JWT_SECRET=dev-jwt-secret-change-in-production
API_KEY=dev-api-key-change-in-production

# CORS Configuration
CORS_ORIGIN=http://localhost:3001,http://localhost:19006,http://203.175.11.145:9005

# Rate Limiting
RATE_LIMIT_MAX=100000
RATE_LIMIT_WINDOW_MS=60000

# Logging
LOG_LEVEL=info

# Database (Future use)
DATABASE_URL=postgresql://localhost/iot_dashboard

# Gemini AI Configuration
GEMINI_API_KEY=your-gemini-api-key-here
GEMINI_MODEL=gemini-1.5-pro-latest
GEMINI_MAX_FILE_SIZE=50MB
GEMINI_SUPPORTED_FORMATS=image/jpeg,image/png,image/gif,image/webp,video/mp4,video/mov,video/avi,audio/mp3,audio/wav,audio/aac


PRIVATE_SERVER_PORT=3001
PUBLIC_PORT=9005
PUBLIC_VPS_IP=
SSH_USER=
SSH_PASSWORD=

# Python GPU Face Recognition Service
PYTHON_GPU_SERVICE_URL=http://203.175.11.145:9009
PYTHON_GPU_SERVICE_ENABLED=true
```

this is my `iot-image\api_routes.py` :
```
# api_routes.py
import logging
import time
from pathlib import Path
from typing import Optional

from fastapi import (APIRouter, File, Form, HTTPException, Request,
                     UploadFile)
from fastapi.responses import JSONResponse

from config import DATA_DIR, PERMITTED_FACES_DIR, face_recognition_available
from data_store import data_store

logger = logging.getLogger(__name__)
router = APIRouter(prefix="/api/v1")


@router.post("/devices/register")
async def register_device_endpoint(deviceId: str = Form(...), deviceName: str = Form(...)):
    try:
        device_data = {'id': deviceId, 'name': deviceName, 'status': 'online'}
        registered_device = data_store.register_device(device_data)
        return JSONResponse(content={"success": True, "device": registered_device})
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))


@router.post("/stream/stream")
async def stream_endpoint(image: UploadFile = File(...), deviceId: Optional[str] = Form("unknown")):
    contents = await image.read()
    if not contents:
        raise HTTPException(status_code=400, detail="Empty image file.")
    
    result = await data_store.perform_face_recognition(contents)
    result["deviceId"] = deviceId
    return JSONResponse(content=result)


@router.post("/recognition/add-permitted-face")
async def add_permitted_face(image: UploadFile = File(...), name: str = Form(...)):
    if not face_recognition_available:
        raise HTTPException(status_code=501, detail="Face recognition feature not available.")

    safe_name = "".join(c for c in name if c.isalnum() or c in (' ', '.', '_')).rstrip()
    if not safe_name:
        raise HTTPException(status_code=400, detail="Invalid name provided.")
        
    extension = Path(image.filename).suffix or ".jpg"
    file_path = PERMITTED_FACES_DIR / f"{safe_name}{extension}"
    
    contents = await image.read()
    with open(file_path, "wb") as f:
        f.write(contents)
    
    logger.info(f"Saved new permitted face '{name}' to {file_path}")
    data_store.load_permitted_faces() # Reload faces
    
    return JSONResponse(content={"success": True, "message": f"Permitted face '{name}' added."})

@router.post("/recognize")
async def recognize_endpoint(image: UploadFile = File(...)):
    """
    This endpoint is specifically for face recognition requests from the backend.
    """
    logger.info(f"Received request for /api/v1/recognize. File: {image.filename}")
    start_time = time.time()
    
    contents = await image.read()
    if not contents:
        raise HTTPException(status_code=400, detail="Uploaded file is empty.")

    # Perform the recognition
    result = await data_store.perform_face_recognition(contents)
    
    # Add processing time to the response for logging and debugging
    processing_time = time.time() - start_time
    result["processing_time"] = round(processing_time, 4)
    
    logger.info(f"Returning recognition result: {result}")
    return JSONResponse(content=result)

@router.get("/devices")
async def get_all_devices_endpoint():
    devices_list = data_store.get_all_devices()
    return JSONResponse(content={"success": True, "devices": devices_list})

# A simple root endpoint for the router
@router.get("/")
async def api_root():
    return {"message": "API v1 is active"}
```

this is my `iot-image\config.py` : 
```
# config.py
import logging
import os
import sys
from pathlib import Path

from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# --- Core Configuration ---
HOST = os.getenv('HOST', '0.0.0.0')
PORT = int(os.getenv('PORT', '9001'))
RELOAD_DEBUG = os.getenv('RELOAD_DEBUG', 'False').lower() == 'true'
UVICORN_LOG_LEVEL = os.getenv('UVICORN_LOG_LEVEL', 'info').lower()

# --- Logging Configuration ---
LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO").upper()
logging.basicConfig(
    level=LOG_LEVEL,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# --- Directory Paths ---
# Assumes this config.py is in a subdirectory (e.g., 'src'), so we go up one level.
# Change if your file structure is different.
BASE_DIR = Path(__file__).resolve().parent
DATA_DIR = BASE_DIR / "data"
RECORDINGS_DIR = BASE_DIR / "recordings"
PERMITTED_FACES_DIR = BASE_DIR / "permitted_faces"

# Create directories on startup
for directory in [DATA_DIR, RECORDINGS_DIR, PERMITTED_FACES_DIR]:
    directory.mkdir(parents=True, exist_ok=True)
    logger.info(f"Ensured directory exists: {directory}")

# --- SSH Tunnel Configuration ---
SSH_HOST = os.getenv("PUBLIC_VPS_IP")
SSH_CONNECTION_PORT = int(os.getenv("SSH_SERVER_PORT", "22"))
SSH_USER = os.getenv("SSH_USER")
SSH_PASSWORD = os.getenv("SSH_PASSWORD")
SSH_PUBLIC_PORT = int(os.getenv('PUBLIC_PORT', '9009'))
SSH_PRIVATE_PORT = int(os.getenv('PRIVATE_SERVER_PORT') or PORT)

# --- Optional Library Availability ---
try:
    import cv2
    cv2_available = True
except ImportError:
    cv2 = None
    cv2_available = False

try:
    import numpy as np
    numpy_available = True
except ImportError:
    np = None
    numpy_available = False

try:
    import face_recognition
    face_recognition_available = True
except ImportError:
    face_recognition = None
    face_recognition_available = False
    
try:
    import uvicorn
    uvicorn_available = True
except ImportError:
    uvicorn = None
    uvicorn_available = False

logger.info(f"Logging initialized with level: {LOG_LEVEL}")
logger.info(f"OpenCV (cv2) available: {cv2_available}")
logger.info(f"NumPy (np) available: {numpy_available}")
logger.info(f"face_recognition available: {face_recognition_available}")
```

this is my `iot-image\data_store.py` : 
```
# data_store.py
import logging
import time
from typing import Any, Dict, List, Optional

# Import dependencies and config variables from the config module
from config import (PERMITTED_FACES_DIR, cv2, cv2_available,
                    face_recognition, face_recognition_available, np,
                    numpy_available)

logger = logging.getLogger(__name__)

class DataStore:
    def __init__(self):
        self.devices: Dict[str, Dict[str, Any]] = {}
        self.permitted_face_encodings: List[Any] = []
        self.permitted_face_names: List[str] = []
        logger.info("DataStore initialized.")

    def load_permitted_faces(self):
        logger.info("Loading permitted faces...")
        self.permitted_face_encodings.clear()
        self.permitted_face_names.clear()

        if not all([face_recognition_available, cv2_available, numpy_available]):
            logger.warning("A required library (face_recognition, cv2, or numpy) is not available. Skipping face loading.")
            return

        if not PERMITTED_FACES_DIR.exists():
            logger.warning(f"Permitted faces directory does not exist: {PERMITTED_FACES_DIR}")
            return
            
        loaded_count = 0
        for image_path in PERMITTED_FACES_DIR.glob("*.[jp][pn]g"):
            try:
                image = face_recognition.load_image_file(str(image_path))
                encodings = face_recognition.face_encodings(image)
                if encodings:
                    self.permitted_face_encodings.append(encodings[0])
                    self.permitted_face_names.append(image_path.stem)
                    loaded_count += 1
                    logger.info(f"Loaded permitted face: {image_path.stem}")
                else:
                    logger.warning(f"No face found in {image_path.name}")
            except Exception as e:
                logger.error(f"Failed to process {image_path.name}: {e}", exc_info=True)
        
        logger.info(f"Finished loading permitted faces. Total loaded: {loaded_count}")

    async def perform_face_recognition(self, image_bytes: bytes) -> Dict[str, Any]:
        if not all([face_recognition_available, cv2_available, numpy_available]):
            return {"status": "error", "message": "Face recognition feature not available."}
        
        try:
            image_array = np.frombuffer(image_bytes, np.uint8)
            image_bgr = cv2.imdecode(image_array, cv2.IMREAD_COLOR)
            if image_bgr is None:
                raise ValueError("Failed to decode image.")
            
            # Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses)
            image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)

            face_locations = face_recognition.face_locations(image_rgb)
            if not face_locations:
                return {"status": "no_face_detected", "faces_detected": 0}

            face_encodings = face_recognition.face_encodings(image_rgb, face_locations)
            
            best_match_name = "Unknown"
            best_confidence = 0.0

            if self.permitted_face_encodings:
                for face_encoding in face_encodings:
                    matches = face_recognition.compare_faces(self.permitted_face_encodings, face_encoding)
                    face_distances = face_recognition.face_distance(self.permitted_face_encodings, face_encoding)
                    
                    if len(face_distances) > 0:
                        best_match_index = np.argmin(face_distances)
                        if matches[best_match_index]:
                            confidence = 1 - face_distances[best_match_index]
                            if confidence > best_confidence:
                                best_confidence = confidence
                                best_match_name = self.permitted_face_names[best_match_index]

            if best_match_name != "Unknown":
                return {
                    "status": "permitted_face",
                    "recognizedAs": best_match_name,
                    "confidence": best_confidence,
                    "faces_detected": len(face_locations)
                }
            else:
                 return {
                    "status": "unknown_face",
                    "recognizedAs": None,
                    "confidence": 0.0,
                    "faces_detected": len(face_locations)
                }
        except Exception as e:
            logger.error(f"Error during face recognition: {e}", exc_info=True)
            return {"status": "error", "message": str(e)}

    def register_device(self, device_data: Dict[str, Any]) -> Dict[str, Any]:
        device_id = device_data.get('id')
        if not device_id:
            raise ValueError("Device ID is required.")
        
        current_time_ms = time.time() * 1000
        if device_id in self.devices:
            self.devices[device_id].update(device_data)
            self.devices[device_id]['lastSeen'] = current_time_ms
        else:
            device_data['lastSeen'] = current_time_ms
            self.devices[device_id] = device_data
        
        logger.info(f"Registered/updated device: {device_id}")
        return self.devices[device_id]

    def get_all_devices(self) -> List[Dict[str, Any]]:
        return list(self.devices.values())

# Create a single, shared instance of the DataStore
data_store = DataStore()
```

this is my `iot-image\main.py` :
```
import logging
import sys
import time
from contextlib import asynccontextmanager

from fastapi import FastAPI, Request
from fastapi.responses import JSONResponse
from fastapi.staticfiles import StaticFiles

# Import from our new modules
import config
from api_routes import router as api_router
from data_store import data_store
from ssh_tunnel import (create_ssh_tunnel, get_tunnel_instance,
                        stop_ssh_tunnel)

logger = logging.getLogger(__name__)

@asynccontextmanager
async def lifespan(app: FastAPI):
    # --- Startup Logic ---
    logger.info("Application starting up...")
    
    # Load permitted faces into memory
    data_store.load_permitted_faces()

    # Start SSH reverse tunnel if configured
    if config.SSH_HOST and config.SSH_USER:
        logger.info("Attempting to start SSH reverse tunnel...")
        create_ssh_tunnel(
            public_vps_ip=config.SSH_HOST,
            ssh_server_port=config.SSH_CONNECTION_PORT,
            ssh_user=config.SSH_USER,
            ssh_password=config.SSH_PASSWORD,
            public_port=config.SSH_PUBLIC_PORT,
            private_server_port=config.SSH_PRIVATE_PORT
        )
    else:
        logger.warning("SSH tunnel environment variables not set. Skipping tunnel.")
    
    app.state.start_time = time.time()
    yield
    # --- Shutdown Logic ---
    logger.info("Application shutting down...")
    stop_ssh_tunnel()
    logger.info("Shutdown complete.")

# --- FastAPI App Initialization ---
app = FastAPI(
    title="IoT Backend GPU Server",
    version="1.0.0",
    lifespan=lifespan
)

# --- Global Exception Handler ---
@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    logger.error(f"Unhandled exception for request {request.url.path}: {exc}", exc_info=True)
    return JSONResponse(
        status_code=500,
        content={"status": "error", "message": "An internal server error occurred."}
    )

# --- Include Routers and Static Files ---
app.include_router(api_router)
app.mount("/data", StaticFiles(directory=config.DATA_DIR), name="data")

# --- Root and Health Check Endpoints ---
@app.get("/")
async def root():
    return {"message": "Server is running."}

@app.get("/health")
async def health_check():
    tunnel = get_tunnel_instance()
    return {
        "status": "healthy",
        "uptime_seconds": round(time.time() - app.state.start_time),
        "face_recognition_ready": config.face_recognition_available,
        "ssh_tunnel_active": tunnel.is_active if tunnel else False
    }

# --- Main Execution ---
if __name__ == "__main__":
    if not config.uvicorn_available:
        logger.critical("Uvicorn is not installed. Please run: pip install uvicorn[standard]")
        sys.exit(1)
        
    logger.info(f"Starting server on {config.HOST}:{config.PORT}")
    config.uvicorn.run(
        "main:app",
        host=config.HOST,
        port=config.PORT,
        reload=config.RELOAD_DEBUG,
        log_level=config.UVICORN_LOG_LEVEL.lower()
    )
```

this is `iot-image\.env` : 
```
# Python GPU Face Recognition Service Configuration
PORT=9001
HOST=0.0.0.0
DEBUG=True

# SSH Reverse Tunnel Configuration
PUBLIC_VPS_IP=203.175.11.145
PUBLIC_PORT=9009
PRIVATE_SERVER_PORT=9001
SSH_USER=
SSH_PASSWORD=

# Face Recognition Configuration
FACE_RECOGNITION_ENABLED=True
FACE_DETECTION_MODEL=hog
FACE_RECOGNITION_TOLERANCE=0.5

# Backend Express Service Configuration
EXPRESS_BACKEND_URL=http://203.175.11.145:9005
EXPRESS_BACKEND_API_KEY=dev-api-key-change-in-production

# Logging
LOG_LEVEL=INFO
```


the image i sent is the result of your code.
now i want you to make it so that it will detect face in an image like it. the image sent will be something like it. 
so first
in the BACKEND please make the image better. not that ugly. please find or use existing or new library to make the image better.
and when hitting the python service image. please make the image face recognition in python to also handle that image like that.
remember that the arduino c++ code is not wrong. the problem is backend. please improve it far far further.
or when the arduino send data. it can also send some data that will help the backend make the image far far better.
basically the arduino right now is fine, but if there's a way to upgrade the image in the BACKEND, but using data from arduino, you can send it. but mostly modify the backend and python to handle it.